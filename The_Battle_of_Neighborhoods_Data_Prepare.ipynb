{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Pattern Between Taxi Engagement Time and Neighborhoods in New York City (Data Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data\n",
    "The whole practice will use several different data sources.  \n",
    "\n",
    "**Taxi Trip Data**  \n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page provides \"Data of trips taken by taxis and for-hire vehicles in New York City\".  \"The yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts\".  I'll use pick-up/drop-off dates/times and locations in green taxi trip data for this practice.\n",
    "\n",
    "**Foursquare Location Data**  \n",
    "The taxi pick-up/drop-off locations nearby venue info can be queried via Foursquare API.  And the nearby venue info will be used for neighborhood segmentation and clustering.\n",
    "\n",
    "**Latitude and Longitude of Location**  \n",
    "The code to request Latitude and longitude information would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder\n",
    "\n",
    "g = geocoder.arcgis('Alphabet City, Manhattan')\n",
    "latitude = g.lat\n",
    "longitude = g.lng\n",
    "print('The geograpical coordinate of East Harlem North, Manhattan are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, import libraries and read data from nyc website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "np.random.seed(0)\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import folium # map rendering library\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data = pd.read_csv('https://nyc-tlc.s3.amazonaws.com/trip+data/yellow_tripdata_2019-12.csv', low_memory=False)\n",
    "print(trip_data.head())\n",
    "\n",
    "zone_lookup = pd.read_csv('https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv')\n",
    "print(zone_lookup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step, insert Borough and Zone info into trip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PULocation_Borough = []\n",
    "PULocation_Zone = []\n",
    "DOLocation_Borough = []\n",
    "DOLocation_Zone = []\n",
    "\n",
    "for t in trip_data.PULocationID:\n",
    "    PULocation_Borough.append(zone_lookup[zone_lookup['LocationID'] == t].Borough.values[0])\n",
    "    PULocation_Zone.append(zone_lookup[zone_lookup['LocationID'] == t].Zone.values[0])\n",
    "for t in trip_data.DOLocationID:\n",
    "    DOLocation_Borough.append(zone_lookup[zone_lookup['LocationID'] == t].Borough.values[0])\n",
    "    DOLocation_Zone.append(zone_lookup[zone_lookup['LocationID'] == t].Zone.values[0])\n",
    "\n",
    "trip_data.insert(loc=6,column='PU_Borough',value=PULocation_Borough)\n",
    "trip_data.insert(loc=7,column='PU_Zone',value=PULocation_Zone)\n",
    "trip_data.insert(loc=9,column='DO_Borough',value=DOLocation_Borough)\n",
    "trip_data.insert(loc=10,column='DO_Zone',value=DOLocation_Zone)\n",
    "\n",
    "trip_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third step, get latitude and longitude data for each taxi zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder\n",
    "\n",
    "taxi_zone_location_list = []\n",
    "for i in range(len(zone_lookup)):\n",
    "    row = str(zone_lookup.iloc[i].Zone) + ', ' + zone_lookup.iloc[i].Borough \n",
    "    taxi_zone_location_list.append(row)\n",
    "\n",
    "taxi_zone_location_dic = {'location':taxi_zone_location_list}\n",
    "taxi_zone_location = pd.DataFrame(data=taxi_zone_location_dic)\n",
    "print(taxi_zone_location.head())\n",
    "\n",
    "lat = []\n",
    "lng = []\n",
    "for i in range(len(taxi_zone_location)):\n",
    "    row = taxi_zone_location.iloc[i].location + ', New York City, NY'\n",
    "    xy = geocoder.arcgis(row)\n",
    "    lat.append(xy.lat)\n",
    "    lng.append(xy.lng)\n",
    "taxi_zone_location.insert(loc=1,column='Latitude',value=lat)\n",
    "taxi_zone_location.insert(loc=2,column='Longitude',value=lng)\n",
    "\n",
    "# Save the location data into a csv file, so it can be used later.\n",
    "taxi_zone_location.to_csv('taxi_zone_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth step, prepare venues data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = # your Foursquare ID\n",
    "CLIENT_SECRET = # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100 # limit of number of venues returned by Foursquare API\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        # retry 10 times if request returns null\n",
    "        for i in range(0,10):\n",
    "            results = requests.get(url).json()\n",
    "            if results[\"response\"]['groups'] is not None:\n",
    "                #print('not None')\n",
    "                #print(results[\"response\"])\n",
    "                break\n",
    "            #print('None')\n",
    "        results = results[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "taxi_zone_venues = getNearbyVenues(names=taxi_zone_location['location'],\n",
    "                                   latitudes=taxi_zone_location['Latitude'],\n",
    "                                   longitudes=taxi_zone_location['Longitude']\n",
    "                                  )\n",
    "\n",
    "taxi_zone_venues.to_csv('taxi_zone_venues_500.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth step, cluster venues by neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_venues.groupby('Neighborhood').count()\n",
    "print('There are {} uniques categories.'.format(len(taxi_zone_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "taxi_zone_onehot = pd.get_dummies(taxi_zone_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "taxi_zone_onehot['Neighborhood'] = taxi_zone_venues['Neighborhood'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [taxi_zone_onehot.columns[-1]] + list(taxi_zone_onehot.columns[:-1])\n",
    "taxi_zone_onehot = taxi_zone_onehot[fixed_columns]\n",
    "\n",
    "taxi_zone_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_grouped = taxi_zone_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "taxi_zone_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] = taxi_zone_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(taxi_zone_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(taxi_zone_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_venues_sorted.to_csv('neighborhoods_venues_sorted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sixth step, format pickup time and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_hour = trip_data.tpep_pickup_datetime.apply(lambda x: time.strptime(x,\"%Y-%m-%d %H:%M:%S\").tm_hour)\n",
    "pickup_hour.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_weekday = trip_data.tpep_pickup_datetime.apply(lambda x: time.strptime(x,\"%Y-%m-%d %H:%M:%S\").tm_wday)\n",
    "pickup_weekday.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data.insert(loc=2,column='lpep_pickup_hour',value=pickup_hour)\n",
    "trip_data.insert(loc=2,column='lpep_pickup_wday',value=pickup_weekday)\n",
    "trip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_pickup_group = trip_data.groupby(['PULocationID','lpep_pickup_hour']).count()\n",
    "trip_pickup_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_pickup = pd.DataFrame(data={'0':[],'1':[],'2':[],'3':[],'4':[],'5':[],'6':[],'7':[],'8':[],'9':[],'10':[],'11':[],'12':[],'13':[],'14':[],'15':[],'16':[],'17':[],'18':[],'19':[],'20':[],'21':[],'22':[],'23':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,266):\n",
    "    t = trip_pickup_group.query('PULocationID == '+str(i))\n",
    "    s = 0\n",
    "    for j in range(0,23):\n",
    "        tt = t.query('lpep_pickup_hour == '+str(j))\n",
    "        if tt.empty:\n",
    "            ttt = 0\n",
    "        else:\n",
    "            ttt = tt.VendorID.values[0]\n",
    "        s = s + ttt\n",
    "    ss = []\n",
    "    for j in range(0,24):\n",
    "        tt = t.query('lpep_pickup_hour == '+str(j))\n",
    "        if tt.empty or s == 0:\n",
    "            ss.append(0.0)\n",
    "        else:\n",
    "            ss.append(tt.VendorID.values[0]*1.0/s)\n",
    "    gtp = pd.DataFrame(data={'0':ss[0],'1':ss[1],'2':ss[2],'3':ss[3],'4':ss[4],'5':ss[5],'6':ss[6],'7':ss[7],'8':ss[8],'9':ss[9]\n",
    "                             ,'10':ss[10],'11':ss[11],'12':ss[12],'13':ss[13],'14':ss[14],'15':ss[15],'16':ss[16],'17':ss[17]\n",
    "                             ,'18':ss[18],'19':ss[19],'20':ss[20],'21':ss[21],'22':ss[22],'23':ss[23]}, index=[i])\n",
    "    trip_pickup = trip_pickup.append(gtp,ignore_index=True)\n",
    "\n",
    "trip_pickup.to_csv('trip_pickup.csv', index=False)\n",
    "trip_pickup.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
